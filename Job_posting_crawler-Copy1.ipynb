{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Posting Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code that will pull each job posting for a specific job title (currently just Data Scientist) in a specific location (or Nationally) and return / plot the percentage of the postings that have certain user-defined keywords that are passed to the code.  \n",
    "\n",
    "NOTE:  I got this idea from https://jessesw.com/Data-Science-Skills/.  Obviously, just using his code would be of no real benefit to me, as I wanted to use the idea to help better my skills with scraping data from HTML files.  So, I used his idea and developed my own code from scratch.  I also modified the overall process a bit to better fit my needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib2 # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "\n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "\n",
    "\n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "\n",
    "\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "\n",
    "\n",
    "\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "\n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "\n",
    "\n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "\n",
    "\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "\n",
    "\n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "\n",
    "\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = text_cleaner('http://www.indeed.com/rc/clk?jk=f54117307d3f6805')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['operations',\n",
       " 'knowledgeable',\n",
       " 'hands',\n",
       " 'insight',\n",
       " 'results',\n",
       " 'feature',\n",
       " 'performance',\n",
       " 'scientist',\n",
       " 'including',\n",
       " 'solutions',\n",
       " 'statistical',\n",
       " 'innovative',\n",
       " 'jobpowered',\n",
       " 'decisions',\n",
       " 'networks',\n",
       " 'relationships',\n",
       " 'web',\n",
       " 'group',\n",
       " 'knowledge',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "\n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "\n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "\n",
    "    final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "\n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "\n",
    "    base_url = 'http://www.indeed.com'\n",
    "\n",
    "\n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html) # Get the html from the first page\n",
    "\n",
    "    # Now find out how many jobs there were\n",
    "\n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    print job_numbers\n",
    "\n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "\n",
    "    print 'There were', total_num_jobs, 'jobs found,', city_title # Display how many jobs were found\n",
    "\n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "\n",
    "    for i in xrange(1,num_pages+1): # Loop through all of our search result pages\n",
    "        print 'Getting page', i\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "\n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page\n",
    "\n",
    "        page_obj = BeautifulSoup(html_page) # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "\n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "\n",
    "        job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "\n",
    "\n",
    "        for j in xrange(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "    print 'Done with collecting the job postings!'    \n",
    "    print 'There were', len(job_descriptions), 'jobs successfully found.'\n",
    "\n",
    "\n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "\n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "\n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "\n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "\n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "\n",
    "\n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "\n",
    "\n",
    "\n",
    "    final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                                # dataframe \n",
    "\n",
    "    # Change the values to reflect a percentage of the postings \n",
    "\n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "\n",
    "    # Sort the data for plotting purposes\n",
    "\n",
    "    final_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\n",
    "\n",
    "    # Get it ready for a bar plot\n",
    "\n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                            title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "\n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "\n",
    "\n",
    "    return fig, final_frame # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a3598d043eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseattle_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Seattle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'WA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "fig, seattle_info = skills_info(city = 'Seattle', state = 'WA') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = PdfPages('Seattle_skills.pdf')\n",
    "pp.savefig(fig)\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
